{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "mkG4e3wD14_6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C-LSTM with the embedding layer with the pre-trained glove embeddings\n",
        "class CLSTMClassifier(tf.keras.Model):\n",
        "    def __init__(self, config, embedding_matrix):\n",
        "        super(CLSTMClassifier, self).__init__()\n",
        "        self.max_length = config.max_length\n",
        "        self.num_classes = config.num_classes\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embedding_size = config.embedding_size\n",
        "        self.filter_sizes = list(map(int, config.filter_sizes.split(\",\")))\n",
        "        self.num_filters = config.num_filters\n",
        "        self.num_layers = config.num_layers\n",
        "        self.hidden_size = len(self.filter_sizes) * self.num_filters\n",
        "        self.l2_reg_lambda = config.l2_reg_lambda\n",
        "\n",
        "        # embedding layer initialized with glove embeddings\n",
        "        self.embedding = layers.Embedding(input_dim=self.vocab_size,\n",
        "                                          output_dim=self.embedding_size,\n",
        "                                          input_length=self.max_length,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          trainable=True)\n",
        "\n",
        "        # conv layers for different filter sizes\n",
        "        self.conv_layers = [\n",
        "            layers.Conv2D(filters=self.num_filters,\n",
        "                          kernel_size=(filter_size, self.embedding_size),\n",
        "                          activation='relu', padding='valid')\n",
        "            for filter_size in self.filter_sizes\n",
        "        ]\n",
        "\n",
        "        # lstm layer\n",
        "        self.lstm = layers.LSTM(self.hidden_size, return_sequences=False)\n",
        "\n",
        "        self.dropout = layers.Dropout(rate=config.keep_prob)\n",
        "\n",
        "        self.fc = layers.Dense(self.num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(self.l2_reg_lambda))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_x = inputs\n",
        "        x = self.embedding(input_x)  # [batch_size, max_length, embedding_size]\n",
        "        x = tf.expand_dims(x, -1)    # [batch_size, max_length, embedding_size, 1]\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv_layer in self.conv_layers:\n",
        "            conv = conv_layer(x)\n",
        "            conv = tf.squeeze(conv, 2)  # squeezing out the 'channels' dimension\n",
        "            conv_outputs.append(conv)\n",
        "\n",
        "        # the minimum sequence length across all convolution outputs\n",
        "        min_length = min([conv.shape[1] for conv in conv_outputs])\n",
        "\n",
        "        # trimmikng all convolution outputs to the same sequence length\n",
        "        conv_outputs = [conv[:, :min_length, :] for conv in conv_outputs]\n",
        "\n",
        "        if len(conv_outputs) > 1:\n",
        "            rnn_inputs = tf.concat(conv_outputs, -1)  # concat along the last dimension\n",
        "        else:\n",
        "            rnn_inputs = conv_outputs[0]\n",
        "\n",
        "        # feed it to the LSTM\n",
        "        rnn_outputs = self.lstm(rnn_inputs)\n",
        "\n",
        "        rnn_outputs = self.dropout(rnn_outputs, training=training)\n",
        "\n",
        "        # final output layer\n",
        "        logits = self.fc(rnn_outputs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CizRl-a5124f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNdnxLI-egVZ",
        "outputId": "10df7e6a-8069-45ff-8414-13baf5949345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 112ms/step - accuracy: 0.5708 - loss: 0.8184 - val_accuracy: 0.6430 - val_loss: 0.6244\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.7935 - loss: 0.4546 - val_accuracy: 0.8782 - val_loss: 0.3180\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.8969 - loss: 0.2759 - val_accuracy: 0.8878 - val_loss: 0.2804\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9213 - loss: 0.2275 - val_accuracy: 0.8899 - val_loss: 0.2889\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9408 - loss: 0.1839 - val_accuracy: 0.8842 - val_loss: 0.2919\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9517 - loss: 0.1545 - val_accuracy: 0.8916 - val_loss: 0.3061\n",
            "Epoch 7/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9675 - loss: 0.1207 - val_accuracy: 0.8882 - val_loss: 0.3179\n",
            "Epoch 8/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9754 - loss: 0.1033 - val_accuracy: 0.8874 - val_loss: 0.3264\n",
            "Epoch 9/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9831 - loss: 0.0786 - val_accuracy: 0.8873 - val_loss: 0.3523\n",
            "Epoch 10/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.9863 - loss: 0.0673 - val_accuracy: 0.8801 - val_loss: 0.3750\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8802 - loss: 0.3743\n",
            "Test Loss: 0.3750276267528534, Test Accuracy: 0.8801199793815613\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "MAX_LEN = 500  # max length of sequences (padded or truncated)\n",
        "VOCAB_SIZE = 5000  # the vocabulary\n",
        "EMBEDDING_DIM = 300  # glove embedding dimensions\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "# padding the sequences to ensure uniform input size\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
        "\n",
        "\n",
        "def load_glove_embeddings(glove_file_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding_vector\n",
        "    return embeddings_index\n",
        "\n",
        "def create_embedding_matrix(word_index, glove_embeddings, vocab_size, embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:  # Only consider the top 'vocab_size' words\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "glove_file_path = '/content/drive/MyDrive/glove/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path, EMBEDDING_DIM)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, VOCAB_SIZE, EMBEDDING_DIM)\n",
        "\n",
        "class Config:\n",
        "    max_length = MAX_LEN\n",
        "    num_classes = 2  # imdb is binary classification (positive/negative)\n",
        "    vocab_size = VOCAB_SIZE\n",
        "    embedding_size = EMBEDDING_DIM  # glove embedding dimension\n",
        "    filter_sizes = \"3,4,5\"\n",
        "    num_filters = 64\n",
        "    num_layers = 1\n",
        "    l2_reg_lambda = 0.1\n",
        "    keep_prob = 0.5\n",
        "\n",
        "config = Config()\n",
        "model = CLSTMClassifier(config, embedding_matrix)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUjdpVRutCie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ncUfaGT21onL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING WITH SST (Stanford Sentiment Treebank)"
      ],
      "metadata": {
        "id": "L4Qe7A7N1pTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "train_texts = dataset['train']['sentence']\n",
        "train_labels = dataset['train']['label']\n",
        "val_texts = dataset['validation']['sentence']\n",
        "val_labels = dataset['validation']['label']\n",
        "test_texts = dataset['test']['sentence']\n",
        "test_labels = dataset['test']['label']\n",
        "\n",
        "VOCAB_SIZE = 5000\n",
        "MAX_LEN = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "id": "Dd8CjybK0xcK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQnKjdYA1w5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}