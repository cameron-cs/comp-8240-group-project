{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "mkG4e3wD14_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C-LSTM with the embedding layer with the pre-trained glove embeddings\n",
        "class CLSTMClassifier(tf.keras.Model):\n",
        "    def __init__(self, config, embedding_matrix):\n",
        "        super(CLSTMClassifier, self).__init__()\n",
        "        self.max_length = config.max_length\n",
        "        self.num_classes = config.num_classes\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embedding_size = config.embedding_size\n",
        "        self.filter_sizes = list(map(int, config.filter_sizes.split(\",\")))\n",
        "        self.num_filters = config.num_filters\n",
        "        self.num_layers = config.num_layers\n",
        "        self.hidden_size = len(self.filter_sizes) * self.num_filters\n",
        "        self.l2_reg_lambda = config.l2_reg_lambda\n",
        "\n",
        "        # embedding layer initialized with glove embeddings\n",
        "        self.embedding = layers.Embedding(input_dim=self.vocab_size,\n",
        "                                          output_dim=self.embedding_size,\n",
        "                                          input_length=self.max_length,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          trainable=True)\n",
        "\n",
        "        # conv layers for different filter sizes\n",
        "        self.conv_layers = [\n",
        "            layers.Conv2D(filters=self.num_filters,\n",
        "                          kernel_size=(filter_size, self.embedding_size),\n",
        "                          activation='relu', padding='valid')\n",
        "            for filter_size in self.filter_sizes\n",
        "        ]\n",
        "\n",
        "        # lstm layer\n",
        "        self.lstm = layers.LSTM(self.hidden_size, return_sequences=False)\n",
        "\n",
        "        self.dropout = layers.Dropout(rate=config.keep_prob)\n",
        "\n",
        "        self.fc = layers.Dense(self.num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(self.l2_reg_lambda))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_x = inputs\n",
        "        x = self.embedding(input_x)  # [batch_size, max_length, embedding_size]\n",
        "        x = tf.expand_dims(x, -1)    # [batch_size, max_length, embedding_size, 1]\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv_layer in self.conv_layers:\n",
        "            conv = conv_layer(x)\n",
        "            conv = tf.squeeze(conv, 2)  # squeezing out the 'channels' dimension\n",
        "            conv_outputs.append(conv)\n",
        "\n",
        "        # the minimum sequence length across all convolution outputs\n",
        "        min_length = min([conv.shape[1] for conv in conv_outputs])\n",
        "\n",
        "        # trimmikng all convolution outputs to the same sequence length\n",
        "        conv_outputs = [conv[:, :min_length, :] for conv in conv_outputs]\n",
        "\n",
        "        if len(conv_outputs) > 1:\n",
        "            rnn_inputs = tf.concat(conv_outputs, -1)  # concat along the last dimension\n",
        "        else:\n",
        "            rnn_inputs = conv_outputs[0]\n",
        "\n",
        "        # feed it to the LSTM\n",
        "        rnn_outputs = self.lstm(rnn_inputs)\n",
        "\n",
        "        rnn_outputs = self.dropout(rnn_outputs, training=training)\n",
        "\n",
        "        # final output layer\n",
        "        logits = self.fc(rnn_outputs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CizRl-a5124f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "MAX_LEN = 500  # max length of sequences (padded or truncated)\n",
        "VOCAB_SIZE = 5000  # the vocabulary\n",
        "EMBEDDING_DIM = 300  # glove embedding dimensions"
      ],
      "metadata": {
        "id": "ZrKR13AaUnEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPtbI4RCu9sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5mBPV0eu9qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9q1KZXSu9n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1Cd1gO_u9kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING THE MODEL ON THE IMDB DATASET"
      ],
      "metadata": {
        "id": "KNqX-htjuzK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNdnxLI-egVZ",
        "outputId": "9edd56ed-23e2-4a16-a745-8f72960524d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 100ms/step - accuracy: 0.5800 - loss: 0.8195 - val_accuracy: 0.7029 - val_loss: 0.5979\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 90ms/step - accuracy: 0.8033 - loss: 0.4441 - val_accuracy: 0.8528 - val_loss: 0.3574\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 92ms/step - accuracy: 0.8873 - loss: 0.2962 - val_accuracy: 0.7836 - val_loss: 0.5183\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 93ms/step - accuracy: 0.9085 - loss: 0.2516 - val_accuracy: 0.8848 - val_loss: 0.3031\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 95ms/step - accuracy: 0.9355 - loss: 0.2020 - val_accuracy: 0.8888 - val_loss: 0.3031\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 96ms/step - accuracy: 0.9487 - loss: 0.1719 - val_accuracy: 0.8908 - val_loss: 0.2935\n",
            "Epoch 7/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 96ms/step - accuracy: 0.9584 - loss: 0.1505 - val_accuracy: 0.8886 - val_loss: 0.3251\n",
            "Epoch 8/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 96ms/step - accuracy: 0.9718 - loss: 0.1192 - val_accuracy: 0.8853 - val_loss: 0.3250\n",
            "Epoch 9/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 96ms/step - accuracy: 0.9806 - loss: 0.0967 - val_accuracy: 0.8670 - val_loss: 0.4402\n",
            "Epoch 10/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 96ms/step - accuracy: 0.9833 - loss: 0.0875 - val_accuracy: 0.8833 - val_loss: 0.3945\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.8821 - loss: 0.3971\n",
            "Test Loss: 0.3944534659385681, Test Accuracy: 0.8833199739456177\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "# padding the sequences to ensure uniform input size\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
        "\n",
        "\n",
        "def load_glove_embeddings(glove_file_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding_vector\n",
        "    return embeddings_index\n",
        "\n",
        "def create_embedding_matrix(word_index, glove_embeddings, vocab_size, embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:  # Only consider the top 'vocab_size' words\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "glove_file_path = '/content/drive/MyDrive/glove/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path, EMBEDDING_DIM)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, VOCAB_SIZE, EMBEDDING_DIM)\n",
        "\n",
        "class Config:\n",
        "    max_length = MAX_LEN\n",
        "    num_classes = 2  # imdb is binary classification (positive/negative)\n",
        "    vocab_size = VOCAB_SIZE\n",
        "    embedding_size = EMBEDDING_DIM  # glove embedding dimension\n",
        "    filter_sizes = \"3,4,5\"\n",
        "    num_filters = 64\n",
        "    num_layers = 1\n",
        "    l2_reg_lambda = 0.1\n",
        "    keep_prob = 0.5\n",
        "\n",
        "config = Config()\n",
        "model = CLSTMClassifier(config, embedding_matrix)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUjdpVRutCie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_TIZ5YCvRbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IG3tcNiJvRZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMvBt2XwNLvd",
        "outputId": "5a460e1b-3a7d-4f5c-9faa-e482e0947ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Vkvog7CvP9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDGwCIQOvP7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gw-YpW-HvP3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING THE MODEL ON THE AMAZON REVIEWS DATASET\n",
        "\n"
      ],
      "metadata": {
        "id": "tTRo1FU6vC70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
        "\n",
        "reviews = [item['text'] for item in dataset['full']]\n",
        "labels = [1 if item['rating'] >= 3 else 0 for item in dataset['full']]\n",
        "\n",
        "print(f\"First review: {reviews[0]}\")\n",
        "print(f\"First label: {labels[0]}\")"
      ],
      "metadata": {
        "id": "DBLje_6XVDXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7303ee2a-e6a4-4e47-a8ab-3e6d179e3c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First review: This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\n",
            "First label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_lengths = [len(review.split()) for review in reviews]\n",
        "avg_review_length = np.mean(review_lengths)\n",
        "max_review_length = np.max(review_lengths)\n",
        "print(f\"Average review length: {avg_review_length}\")\n",
        "print(f\"Maximum review length: {max_review_length}\")\n",
        "\n",
        "tokeniser = Tokenizer()\n",
        "tokeniser.fit_on_texts(reviews)\n",
        "total_unique_words = len(tokeniser.word_index)\n",
        "print(f\"Total unique words in the dataset: {total_unique_words}\")\n",
        "\n",
        "\n",
        "import collections\n",
        "rating_distribution = collections.Counter([item['rating'] for item in dataset['full']])\n",
        "print(f\"Rating distribution: {rating_distribution}\")"
      ],
      "metadata": {
        "id": "xhU3dwD_t59d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "tokeniser = Tokenizer(num_words=10000)  # setting vocab_size to 10,000 as per the updated config\n",
        "tokeniser.fit_on_texts(reviews)  # fitting the tokeniser on the Amazon reviews dataset\n",
        "sequences = tokeniser.texts_to_sequences(reviews)\n",
        "\n",
        "# pad the sequences to the max length of 300\n",
        "x_data = pad_sequences(sequences, maxlen=300)\n",
        "\n",
        "# labels to numpy array\n",
        "y_data = np.array(labels)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "-TkmN1eZjtTP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(glove_file_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding_vector\n",
        "    return embeddings_index\n",
        "\n",
        "def create_embedding_matrix(word_index, glove_embeddings, vocab_size, embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:  # Only consider the top 'vocab_size' words\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "glove_file_path = '/content/drive/MyDrive/glove/glove.6B.300d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path, 300)\n",
        "\n",
        "# embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, 10000, 300)\n"
      ],
      "metadata": {
        "id": "O7_rNweZpWo0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    max_length = 300  # based on the average review length analysis\n",
        "    num_classes = 2  # binary classification (positive/negative)\n",
        "    vocab_size = 10000  # limit vocabulary size to top 10,000 words\n",
        "    embedding_size = 300  #  300-dimensional GloVe embeddings\n",
        "    filter_sizes = \"3,4,5\"  # convolution filter sizes\n",
        "    num_filters = 64  # num of filters for each filter size\n",
        "    num_layers = 1  #  LSTM layer\n",
        "    l2_reg_lambda = 0.1  # L2 regularisation to prevent overfitting\n",
        "    keep_prob = 0.5  # Dropout probability\n",
        "\n",
        "# init the C-LSTM model with the updated configuration and embedding matrix\n",
        "config = Config()\n",
        "model = CLSTMClassifier(config, embedding_matrix)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Xl420RpklCVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1219eb47-3478-4916-bfae-e9b236040059"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "1mPALqcJptVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ae3f46-968e-40c6-b25d-72e5d3e26897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8770/8770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 47ms/step - accuracy: 0.8978 - loss: 0.2671 - val_accuracy: 0.9256 - val_loss: 0.1901\n",
            "Epoch 2/10\n",
            "\u001b[1m8770/8770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 48ms/step - accuracy: 0.9300 - loss: 0.1800 - val_accuracy: 0.9276 - val_loss: 0.1838\n",
            "Epoch 3/10\n",
            "\u001b[1m4132/8770\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 43ms/step - accuracy: 0.9386 - loss: 0.1619"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "LnYJlht8lh_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING THE MODEL ON THE YELP REVIEWS DATASET"
      ],
      "metadata": {
        "id": "YHOI_CJrtn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5rz5KkSpaSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}